{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0e3d70",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d01d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available.\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../../../\")\n",
    "sys.path.append(\"../../../../\")\n",
    "\n",
    "from Clust.setting import influx_setting_KETI as ins\n",
    "from Clust.clust.ingestion.influx import influx_client_v2 as iC\n",
    "from Clust.clust.ingestion.mongo import mongo_client\n",
    "from Clust.clust.ML.tool import meta as ml_meta\n",
    "from Clust.clust.ML.common import ML_api\n",
    "influxdb_client = iC.InfluxClient(ins.CLUSTDataServer2)\n",
    "mongodb_client = mongo_client.MongoClient(ins.CLUSTMetaInfo2)\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device}\" \" is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472041e",
   "metadata": {},
   "source": [
    "# 1. set param from Front End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a7b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "param1 = {\n",
    "    \"ingestion_param_X\" :{\n",
    "        \"bucket_name\": 'integration',\n",
    "        \"ms_name\" : 'regression_energy_cleanLevel4_trainX',\n",
    "        \"feature_list\":['Press_mm_hg', 'RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_5', 'RH_6', 'RH_7',\n",
    "       'RH_8', 'RH_9', 'RH_out', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7',\n",
    "       'T8', 'T9', 'T_out', 'Tdewpoint', 'Visibility', 'Windspeed']\n",
    "    },\n",
    "    \"ingestion_param_y\":{\n",
    "        \"bucket_name\": 'integration',\n",
    "        \"ms_name\" : 'regression_energy_cleanLevel4_trainy',\n",
    "        \"feature_list\":[\"value\"]\n",
    "    },\n",
    "    'data_y_flag' : 'true',\n",
    "    'scaler_param':{\n",
    "        'scaler_flag':'scale', #scale_param,\n",
    "        'scale_method' :'minmax',\n",
    "        'scaler_path' :'./scaler/'\n",
    "    },\n",
    "    \"transform_param\":{\n",
    "        'split_mode' : 'window_split', # 현재 data_y_flag=Ture --> 모두 window_split # data_y = False --> step_split\n",
    "        #step_split일 경우만 past_step과 future_step이 존재\n",
    "        'data_clean_option' : \"false\"\n",
    "    },\n",
    "    \n",
    "    \"model_info\" :{\n",
    "        'model_purpose' : 'regression',\n",
    "        'model_method' : 'LSTM_rg',\n",
    "        'model_name' : \"None\",\n",
    "        'model_tags' : 'tagstest',\n",
    "        'train_parameter' : {\"lr\":0.0001,\"weight_decay\":0.000001,\"n_epochs\":5,\"batch_size\":16},\n",
    "        'model_parameter' : {\"hidden_size\":64,\"num_layers\":2,\"output_dim\":1,\"dropout\":0.1,\"bidirectional\":\"True\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "# forecasting\n",
    "param2 = {\n",
    "    \"ingestion_param_X\" :{\n",
    "        \"bucket_name\": 'integration',\n",
    "        \"ms_name\" : 'forecasting_Hs2SwineFarmWithWeatherTime_cleanLevel4_trainX',\n",
    "        \"feature_list\":['Temperature', 'out_temp','sin_hour']\n",
    "    },\n",
    "    \"ingestion_param_y\":{\n",
    "        \"bucket_name\": 'integration',\n",
    "        \"ms_name\" : 'forecasting_Hs2SwineFarmWithWeatherTime_cleanLevel4_trainy',\n",
    "        \"feature_list\":['Temperature']\n",
    "    },\n",
    "    'data_y_flag' : 'false',\n",
    "    'scaler_param':{\n",
    "        'scaler_flag':'scale', #scale_param,\n",
    "        'scale_method' :'minmax',\n",
    "        'scaler_path' :'./scaler/'\n",
    "    },\n",
    "    \"transform_param\":{\n",
    "        'data_clean_option' : \"false\",\n",
    "        'split_mode' : 'step_split', # 현재 data_y_flag=Ture --> 모두 window_split # data_y = False --> step_split\n",
    "        'past_step':24, #step_split일 경우만 past_step과 future_step이 존재\n",
    "        'future_step':2\n",
    "    },\n",
    "    \n",
    "    \"model_info\" :{\n",
    "        'model_purpose' : 'regression',\n",
    "        'model_method' : 'LSTM_rg',\n",
    "        'model_name' : \"None\",\n",
    "        'model_tags' : 'tagstest',\n",
    "        'train_parameter' : {\"lr\":0.0001,\"weight_decay\":0.000001,\"n_epochs\":5,\"batch_size\":16},\n",
    "        'model_parameter' : {\"hidden_size\":64,\"num_layers\":2,\"output_dim\":1,\"dropout\":0.1,\"bidirectional\":\"True\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "# classification\n",
    "param3 = {\n",
    "    \"ingestion_param_X\" :{\n",
    "        \"bucket_name\": 'integration',\n",
    "        \"ms_name\" : 'classification_actionPattern_cleanLevel0_trainX',\n",
    "        \"feature_list\":['col_0', 'col_1','col_2','col_3','col_4','col_5','col_6','col_7','col_8']\n",
    "    },\n",
    "    \"ingestion_param_y\":{\n",
    "        \"bucket_name\": 'integration',\n",
    "        \"ms_name\" : 'classification_actionPattern_cleanLevel0_trainy',\n",
    "        \"feature_list\":['value']\n",
    "    },\n",
    "    'data_y_flag' : 'true',\n",
    "    'scaler_param':{\n",
    "        'scaler_flag':'scale', #scale_param,\n",
    "        'scale_method' :'minmax',\n",
    "        'scaler_path' :'./scaler/'\n",
    "    },\n",
    "    \"transform_param\":{\n",
    "        'data_clean_option' : \"false\",\n",
    "        'split_mode' : 'window_split' # 현재 data_y_flag=Ture --> 모두 window_split # data_y = False --> step_split\n",
    "    },\n",
    "    \"model_info\" :{\n",
    "        'model_purpose' : 'classification',\n",
    "        'model_method' : 'LSTM_cf',\n",
    "        'model_name' : \"None\",\n",
    "        'model_tags' : 'tagstest',\n",
    "        'train_parameter' : {\"lr\":0.0001,\"weight_decay\":0.000001,\"n_epochs\":5,\"batch_size\":16},\n",
    "        'model_parameter' : {\"hidden_size\":64,\"num_layers\":2,\"output_dim\":1,\"dropout\":0.1,\"bidirectional\":\"True\",\"num_classes\":6, 'rnn_type':'lstm'}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79772cfb",
   "metadata": {},
   "source": [
    "# 2. Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3436645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Backend Prameter Setting\n",
    "from Clust.clust.ML.common import ML_api\n",
    "\n",
    "# parameter tunning\n",
    "param = param2\n",
    "param = ML_api.convert_param_for_backend(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fbfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. train data meta check\n",
    "train_data_info = ML_api.get_train_data_meta (mongodb_client, param['ingestion_param_X'])\n",
    "param['train_data_info'] = train_data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc8d329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucket_name': 'integration', 'ms_name': 'forecasting_Hs2SwineFarmWithWeatherTime_cleanLevel4_trainX', 'feature_list': ['Temperature', 'out_temp', 'sin_hour']}\n",
      "['Temperature', 'out_temp', 'sin_hour']\n",
      "Make New scaler File\n",
      "./scaler/forecasting_Hs2SwineFarmWithWeatherTime_cleanLevel4_trainX/minmax/c1b524a5c854f6e561c4764b1dfb4e6f/scaler.pkl\n",
      "['Temperature']\n",
      "Make New scaler File\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_clean_option': False,\n",
       " 'split_mode': 'step_split',\n",
       " 'past_step': 24,\n",
       " 'future_step': 2,\n",
       " 'nan_process_info': {'type': 'num',\n",
       "  'ConsecutiveNanLimit': 10000,\n",
       "  'totalNaNLimit': 100000},\n",
       " 'max_nan_limit_ratio': 0.5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Data Preparation\n",
    "#train_X_array, train_y_array, val_X_array, val_y_array = ML_api.ML_data_preparation(param, influxdb_client)\n",
    "\n",
    "from Clust.clust.ML.common import ML_pipeline\n",
    "# 1. Oirignla data ingestion\n",
    "data_X, data_y = ML_pipeline.Xy_data_preparation(param['ingestion_param_X'], \n",
    "                                             param['data_y_flag'], \n",
    "                                             param['ingestion_param_y'],\n",
    "                                             'ms_all', \n",
    "                                             influxdb_client)\n",
    "# 2. Scaling\n",
    "dataX_scaled, datay_scaled = ML_pipeline.Xy_data_scaling_train(param['ingestion_param_X']['ms_name'], \n",
    "                                                                                 data_X, \n",
    "                                                                                 param['ingestion_param_y']['ms_name'], \n",
    "                                                                                 data_y, \n",
    "                                                                                 param['scaler_param'])\n",
    "\n",
    "\n",
    "\n",
    "# 3.clean column\n",
    "dataX_scaled = ML_pipeline.clean_low_quality_column(dataX_scaled, param['transform_param'])\n",
    "\n",
    "param['transform_param']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8d616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 4. split train/Val\n",
    "split_ratio = 0.8\n",
    "train_X, val_X, train_y, val_y, param['transform_param']= ML_pipeline.split_data_by_mode(dataX_scaled, \n",
    "                                                                                         datay_scaled, \n",
    "                                                                                         split_ratio, \n",
    "                                                                                         param['transform_param'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2067660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>out_temp</th>\n",
       "      <th>sin_hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-01 00:00:00+00:00</th>\n",
       "      <td>0.513455</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 00:10:00+00:00</th>\n",
       "      <td>0.513455</td>\n",
       "      <td>0.206197</td>\n",
       "      <td>0.521568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 00:20:00+00:00</th>\n",
       "      <td>0.513455</td>\n",
       "      <td>0.207265</td>\n",
       "      <td>0.543137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 00:30:00+00:00</th>\n",
       "      <td>0.514182</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.564705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 00:40:00+00:00</th>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.209402</td>\n",
       "      <td>0.586273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02 12:50:00+00:00</th>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.592415</td>\n",
       "      <td>0.392159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02 13:00:00+00:00</th>\n",
       "      <td>0.731636</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.370590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02 13:10:00+00:00</th>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.559295</td>\n",
       "      <td>0.350492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02 13:20:00+00:00</th>\n",
       "      <td>0.724364</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.330394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02 13:30:00+00:00</th>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.524038</td>\n",
       "      <td>0.310295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4258 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Temperature  out_temp  sin_hour\n",
       "time                                                      \n",
       "2021-02-01 00:00:00+00:00     0.513455  0.205128  0.500000\n",
       "2021-02-01 00:10:00+00:00     0.513455  0.206197  0.521568\n",
       "2021-02-01 00:20:00+00:00     0.513455  0.207265  0.543137\n",
       "2021-02-01 00:30:00+00:00     0.514182  0.208333  0.564705\n",
       "2021-02-01 00:40:00+00:00     0.528000  0.209402  0.586273\n",
       "...                                ...       ...       ...\n",
       "2021-03-02 12:50:00+00:00     0.736000  0.592415  0.392159\n",
       "2021-03-02 13:00:00+00:00     0.731636  0.576923  0.370590\n",
       "2021-03-02 13:10:00+00:00     0.728000  0.559295  0.350492\n",
       "2021-03-02 13:20:00+00:00     0.724364  0.541667  0.330394\n",
       "2021-03-02 13:30:00+00:00     0.728000  0.524038  0.310295\n",
       "\n",
       "[4258 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32999ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.3203125\n",
      "1065\n"
     ]
    }
   ],
   "source": [
    "print(len(val_X)/128)\n",
    "print(len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a7ce2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan_limit_num:  12\n",
      "Original num: 4258 Final num: 4164 NaN num: 94\n",
      "nan_limit_num:  12\n",
      "Original num: 1065 Final num: 1039 NaN num: 26\n"
     ]
    }
   ],
   "source": [
    "# 5. Transform array style\n",
    "train_X_array, train_y_array = ML_pipeline.transform_data_by_split_mode(param['transform_param'], \n",
    "                                                                        train_X, \n",
    "                                                                        train_y)\n",
    "val_X_array, val_y_array = ML_pipeline.transform_data_by_split_mode(param['transform_param'], \n",
    "                                                                    val_X, \n",
    "                                                                    val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6624e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 64, 'num_layers': 2, 'output_dim': 1, 'dropout': 0.1, 'bidirectional': True}\n",
      "./Models/LSTM_rg/forecasting_Hs2SwineFarmWithWeatherTime_cleanLevel4_trainX_regression_LSTM_rg_/forecasting_Hs2SwineFarmWithWeatherTime_cleanLevel4_trainX/model.pkl\n",
      "Start training model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4.Training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mML_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mML_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtrain_y_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_X_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Clust/clust/ML/common/ML_api.py:185\u001b[0m, in \u001b[0;36mML_training\u001b[0;34m(train_X_array, train_y_array, val_X_array, val_y_array, param)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# model training\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# input 순서 일관되도록 펑션 수정\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_purpose\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m:    \n\u001b[0;32m--> 185\u001b[0m     \u001b[43mML_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLUST_regresstion_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtrain_y_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mval_X_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mval_y_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_info\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_purpose\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    192\u001b[0m     ML_pipeline\u001b[38;5;241m.\u001b[39mCLUST_classification_train(train_X_array, \n\u001b[1;32m    193\u001b[0m                                            train_y_array, \n\u001b[1;32m    194\u001b[0m                                            val_X_array, \n\u001b[1;32m    195\u001b[0m                                            val_y_array, \n\u001b[1;32m    196\u001b[0m                                            param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_info\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/code/Clust/clust/ML/common/ML_pipeline.py:201\u001b[0m, in \u001b[0;36mCLUST_regresstion_train\u001b[0;34m(train_X_array, train_y_array, val_X_array, val_y_array, model_info)\u001b[0m\n\u001b[1;32m    199\u001b[0m rml\u001b[38;5;241m.\u001b[39mset_model(model_method, model_parameter)\n\u001b[1;32m    200\u001b[0m rml\u001b[38;5;241m.\u001b[39mset_data(train_X_array, train_y_array, val_X_array, val_y_array)\n\u001b[0;32m--> 201\u001b[0m \u001b[43mrml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m rml\u001b[38;5;241m.\u001b[39msave_best_model(model_file_path)\n",
      "File \u001b[0;32m~/code/Clust/clust/ML/regression/train.py:83\u001b[0m, in \u001b[0;36mRegressionTrain.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Clust/clust/ML/regression/clust_models/rnn_clust.py:69\u001b[0m, in \u001b[0;36mRNNClust.train\u001b[0;34m(self, train_params, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m     67\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mview([batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_features])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     68\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mview([batch_size, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 69\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     batch_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     71\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(batch_losses)\n",
      "File \u001b[0;32m~/code/Clust/clust/ML/regression/clust_models/rnn_clust.py:291\u001b[0m, in \u001b[0;36mRNNClust._train_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Makes predictions\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Computes loss\u001b[39;00m\n\u001b[1;32m    294\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y, yhat)\n",
      "File \u001b[0;32m~/.conda/envs/sejong/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/Clust/clust/ML/regression/models/rnn.py:66\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m     c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_directions \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# We need to detach as we are doing truncated backpropagation through time (BPTT)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# If we don't, we'll backprop all the way to the start even after going through another batch\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Forward propagation by passing in the input, hidden state, and cell state into the model\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     out, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgru\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     68\u001b[0m     out, h0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(x, h0\u001b[38;5;241m.\u001b[39mdetach())\n",
      "File \u001b[0;32m~/.conda/envs/sejong/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/sejong/lib/python3.8/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu"
     ]
    }
   ],
   "source": [
    "# 4.Training\n",
    "param = ML_api.ML_training(train_X_array,  train_y_array, val_X_array, val_y_array, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save meta\n",
    "from Clust.clust.ML.tool import meta\n",
    "#ml_meta.save_model_meta_into_mongodb(mongodb_client, param)\n",
    "meta.save_model_meta_into_local(json_file_path, model_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd18151",
   "metadata": {},
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d63758",
   "metadata": {},
   "outputs": [],
   "source": [
    "param['model_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639eba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
