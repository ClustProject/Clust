{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95790ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "sys.path.append(\"../../../../\")\n",
    "\n",
    "from Clust.clust.ML.tool import data as ml_data\n",
    "from Clust.clust.ML.tool import model as ml_model\n",
    "from Clust.clust.ML.tool import clean as ml_clean\n",
    "from Clust.clust.ML.tool import meta as ml_meta\n",
    "\n",
    "from Clust.setting import influx_setting_KETI as ins\n",
    "from Clust.clust.ingestion.influx import influx_client_v2 as influx_Client\n",
    "from Clust.clust.ingestion.mongo.mongo_client import MongoClient\n",
    "\n",
    "import torch\n",
    "\n",
    "db_client = influx_Client.InfluxClient(ins.CLUSTDataServer2)\n",
    "mongo_client = MongoClient(ins.CLUSTMetaInfo2)\n",
    "\n",
    "#import main_regression as mr\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device}\" \" is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64028082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model methods i.e., 'LSTM_rg', 'GRU_rg', 'CNN_1D_rg', 'LSTM_FCNs_rg', 'FC_rg' \n",
    "model_method = 'GRU_rg'\n",
    "\n",
    "# get integrated data name\n",
    "bucket_name = 'integration'\n",
    "\n",
    "# scaler path\n",
    "scalerPath = './scaler/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e464405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# App_name\n",
    "app_name = \"Hs2SwineFarmWithWeatherTime\" # \"Hs2SwineFarmWithWeatherTime\", \"energy\"\n",
    "step ='train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf28a6",
   "metadata": {},
   "source": [
    "# 2. Training \n",
    "\n",
    "## 2-1. Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02503915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classification_actionPattern_testX_cleanLevel0',\n",
       " 'classification_actionPattern_testy_cleanLevel0',\n",
       " 'classification_actionPattern_trainX_cleanLevel0',\n",
       " 'classification_actionPattern_trainy_cleanLevel0',\n",
       " 'forecasting_Hs2SwineFarmWithWeatherTime_test_cleanLevel0',\n",
       " 'forecasting_Hs2SwineFarmWithWeatherTime_test_cleanLevel4',\n",
       " 'forecasting_Hs2SwineFarmWithWeatherTime_train_cleanLevel0',\n",
       " 'forecasting_Hs2SwineFarmWithWeatherTime_train_cleanLevel4',\n",
       " 'forecasting_gunwiStrawberryWeather_test_cleanLevel0',\n",
       " 'forecasting_gunwiStrawberryWeather_test_cleanLevel4',\n",
       " 'forecasting_gunwiStrawberryWeather_train_cleanLevel0',\n",
       " 'forecasting_gunwiStrawberryWeather_train_cleanLevel4',\n",
       " 'forecasting_strawberryOpen_test_cleanLevel0',\n",
       " 'forecasting_strawberryOpen_test_cleanLevel4',\n",
       " 'forecasting_strawberryOpen_train_cleanLevel0',\n",
       " 'forecasting_strawberryOpen_train_cleanLevel4',\n",
       " 'regression_energy_testX_cleanLevel0',\n",
       " 'regression_energy_testX_cleanLevel4',\n",
       " 'regression_energy_testy_cleanLevel0',\n",
       " 'regression_energy_testy_cleanLevel4',\n",
       " 'regression_energy_trainX_cleanLevel0',\n",
       " 'regression_energy_trainX_cleanLevel4',\n",
       " 'regression_energy_trainy_cleanLevel0',\n",
       " 'regression_energy_trainy_cleanLevel4']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ms_list = db_client.measurement_list(bucket_name)\n",
    "get_ms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f3faa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forecasting_strawberryOpen',\n",
       " 'regression_energy',\n",
       " 'forecasting_Hs2SwineFarmWithWeatherTime',\n",
       " 'forecasting_gunwiStrawberryWeather',\n",
       " 'classification_actionPattern']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_list = mongo_client.get_collection_list(bucket_name)\n",
    "collection_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcec8e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clean_level = 0 \n",
    "if app_name == \"energy\":\n",
    "    model_purpose = 'regression'\n",
    "    dataset_name = model_purpose+'_' +app_name\n",
    "    data_name_X = dataset_name+'_'+step+'X_cleanLevel'+str(data_clean_level)\n",
    "    data_name_y = dataset_name+'_'+step+'y_cleanLevel'+str(data_clean_level)\n",
    "    feature_X_list = ['Press_mm_hg', 'RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_5', 'RH_6', 'RH_7',\n",
    "       'RH_8', 'RH_9', 'RH_out', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7',\n",
    "       'T8', 'T9', 'T_out', 'Tdewpoint', 'Visibility', 'Windspeed']\n",
    "    feature_y_list = ['value']\n",
    "    split_mode =\"window_split\"\n",
    "    \n",
    "elif app_name == \"Hs2SwineFarmWithWeatherTime\":\n",
    "    model_purpose = 'forecasting' \n",
    "    dataset_name = model_purpose+'_' +app_name\n",
    "    data_name_X = dataset_name+'_'+step+'_cleanLevel'+str(data_clean_level)\n",
    "    data_name_y = None\n",
    "    feature_X_list = ['Temperature', 'out_temp','sin_hour']\n",
    "    feature_y_list = ['Temperature']\n",
    "    split_mode = 'step_split'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d51d355",
   "metadata": {},
   "source": [
    "## X Data Ingestion & scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "768ca14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Temperature', 'out_temp', 'sin_hour']\n",
      "Make New scaler File\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion\n",
    "from Clust.clust.data import data_interface\n",
    "ingestion_param = {\n",
    "    \"bucket_name\" : bucket_name,\n",
    "    'ms_name' : data_name_X,\n",
    "    'feature_list' : feature_X_list                              \n",
    "}\n",
    "data_X = data_interface.get_data_result(\"ms_all\", db_client, ingestion_param)\n",
    "data_X = data_X[feature_X_list]\n",
    "\n",
    "# Data Scaling\n",
    "from Clust.clust.ML.tool import scaler\n",
    "scalerParam='scale'\n",
    "scaleMethod='minmax'\n",
    "\n",
    "scalerRootPath_X = os.path.join(scalerPath, data_name_X)\n",
    "dataX_scaled, X_scalerFilePath = scaler.get_data_scaler(scalerParam, scalerRootPath_X, data_X, scaleMethod)                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da26466",
   "metadata": {},
   "source": [
    "## Y Data Ingestion & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e932423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Temperature']\n",
      "Make New scaler File\n"
     ]
    }
   ],
   "source": [
    "if data_name_y:\n",
    "    ingestion_param = {\n",
    "        \"bucket_name\" : bucket_name,\n",
    "        'ms_name' : data_name_y,\n",
    "        'feature_list' : feature_y_list                             \n",
    "    }\n",
    "    data_y = data_interface.get_data_result(\"ms_all\", db_client, ingestion_param)\n",
    "    \n",
    "else: # y 가 없다면 데이터를 만들어야 함\n",
    "    data_y = data_X[feature_y_list]\n",
    "    data_name_y = dataset_name+'_'+step+'y_cleanLevel'+str(data_clean_level)\n",
    " \n",
    "scalerRootPath_y = os.path.join(scalerPath, data_name_y)\n",
    "datay_scaled, y_scalerFilePath = scaler.get_data_scaler(scalerParam, scalerRootPath_y, data_y, scaleMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54c637",
   "metadata": {},
   "source": [
    "## X Feature Selection (column 별로 체크, 문제가 많은 컬럼은 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28752269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ---> 3\n"
     ]
    }
   ],
   "source": [
    "# Clean\n",
    "model_clean = True\n",
    "nan_process_info = {'type':'num', 'ConsecutiveNanLimit':1000000, 'totalNaNLimit':10000000}\n",
    "# 이부분 신경 써야함\n",
    "\n",
    "from Clust.clust.quality.NaN import cleanData\n",
    "CMS = cleanData.CleanData()\n",
    "\n",
    "# Feature Selection 넣을 경우 이 모듈에 코딩\n",
    "dataX_scaled = CMS.get_cleanData_by_removing_column(dataX_scaled, nan_process_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bb5ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_clean:\n",
    "    # model_clean 파라미터를 쓰고 싶은 경우, 아래에서 체크\n",
    "    # interpolation을 하지 않고 그 데이터를 삭제함\n",
    "    pass\n",
    "else:\n",
    "    dataX_scaled = dataX_scaled.interpolate(method='linear')\n",
    "    datay_scaled = datay_scaled.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ae2c2",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c884e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from Clust.clust.transformation.purpose import machineLearning as ML\n",
    "data_meta = mongo_client.get_document_by_json('integration', dataset_name, {'data_name':data_name_X})[0]\n",
    "split_ratio = 0.8\n",
    "# TODO 데이터 나뉘는 부분 추가로 작성된 것 지수님에게 물어봐야 함\n",
    "\n",
    "if split_mode =='window_split':\n",
    "    from datetime import timedelta \n",
    "    # define window size by clust structure\n",
    "    first_date = dataX_scaled.index[0]\n",
    "    window_size = dataX_scaled.loc[first_date:first_date + timedelta(days =0, hours=23, minutes=59, seconds=59)].shape[0]\n",
    "    print(window_size)\n",
    "    train_x, val_x = ML.split_data_by_ratio(dataX_scaled, split_ratio, split_mode, window_size)\n",
    "else:\n",
    "    window_size =None\n",
    "    train_x, val_x = ML.split_data_by_ratio(dataX_scaled, split_ratio, None, None)\n",
    "    \n",
    "train_y, val_y = ML.split_data_by_ratio(datay_scaled, split_ratio, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0495b",
   "metadata": {},
   "source": [
    "## Data Transformation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77429916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 8516 Final Length: 8326 NaN Length: 190\n",
      "Original Length: 2129 Final Length: 2103 NaN Length: 26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "if split_mode =='window_split':\n",
    "    from Clust.clust.transformation.type import DFToNPArray\n",
    "    transformParameter = {\n",
    "        'window_num':window_size\n",
    "    }\n",
    "    train_X_array = DFToNPArray.trans_DF_to_NP_by_windowNum(train_x, transformParameter['window_num'])\n",
    "    val_X_array= DFToNPArray.trans_DF_to_NP_by_windowNum (val_x, transformParameter['window_num'])\n",
    "    train_y_array = train_y.values.reshape(-1)\n",
    "    val_y_array = val_y.values.reshape(-1)\n",
    "    \n",
    "elif split_mode == 'step_split':\n",
    "    transformParameter = {\n",
    "        'future_step': 2,\n",
    "        'past_step': 24\n",
    "    }\n",
    "    train_X_array, train_y_array = ML.trans_by_step_info(train_x, train_y, transformParameter)\n",
    "    val_X_array, val_y_array = ML.trans_by_step_info(val_x, val_y, transformParameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b9d01",
   "metadata": {},
   "source": [
    "## Set Model Parameters & Train Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adac6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN models (RNN, LSTM, GRU) parameters\n",
    "seq_len, input_size = train_X_array.shape[1], train_X_array.shape[2]\n",
    "if model_method == 'LSTM_rg' or model_method == 'GRU_rg':\n",
    "    modelParameter = {\n",
    "        'rnn_type': 'lstm',\n",
    "        'input_size': input_size, \n",
    "        'hidden_size': 64,\n",
    "        'num_layers': 2,\n",
    "        'output_dim': 1, \n",
    "        'dropout': 0.1, \n",
    "        'bidirectional': True\n",
    "    }\n",
    "# CNN_1D model parameters\n",
    "elif model_method == 'CNN_1D_rg':\n",
    "    modelParameter = {\n",
    "    'input_size': input_size,\n",
    "    'seq_len': seq_len,\n",
    "    'output_channels': 64,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 1,\n",
    "    'padding': 0, \n",
    "    'dropout': 0.1\n",
    "    }\n",
    "# LSTM_FCNs model parameters\n",
    "elif model_method == 'LSTM_FCNs_rg':\n",
    "    modelParameter = {\n",
    "    'input_size': input_size,\n",
    "    'num_layers': 2,\n",
    "    'lstm_dropout': 0.4,\n",
    "    'fc_dropout': 0.1\n",
    "    }\n",
    "# FC model parameters\n",
    "elif model_method == 'FC_rg':\n",
    "    modelParameter = {\n",
    "    'input_size': input_size,\n",
    "    'dropout': 0.1,\n",
    "    'bias': True\n",
    "    }\n",
    "    \n",
    "trainParameter = {\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-6, \n",
    "    'device': 'cpu', \n",
    "    'n_epochs': 10, \n",
    "    'batch_size': 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dee514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 모델을 저장할 파일 패스를 생성한다.\n",
    "model_name = None\n",
    "if model_name is None:\n",
    "    model_name = dataset_name + '_' + model_method + '_modelCleanLevel' + str(model_clean)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "from Clust.clust.transformation.general.dataScaler import encode_hash_style\n",
    "trainParameter_encode =  encode_hash_style(str(trainParameter))\n",
    "trainDataPathList = [model_name, data_name_X, trainParameter_encode]\n",
    "modelFilePath = ml_model.get_model_file_path(trainDataPathList, model_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44195f",
   "metadata": {},
   "source": [
    "## 2-5 Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51aab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clust.clust.ML.regression_YK.train import RegressionTrain as RML\n",
    "\n",
    "rml = RML()\n",
    "rml.set_param(trainParameter)\n",
    "rml.set_model(model_method, modelParameter)\n",
    "rml.set_data(train_X_array, train_y_array, val_X_array, val_y_array, )\n",
    "rml.train()\n",
    "rml.save_best_model(modelFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36ad30",
   "metadata": {},
   "source": [
    "## 2-6 Save MetaData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Clust.clust.transformation.general.dataScaler import encode_hash_style\n",
    "modelTags =[\"model_tag_example\"]\n",
    "trainDataType = \"timeseries\"\n",
    "\n",
    "modelInfoMeta ={\n",
    "    \"trainDataInfo\":data_meta,\n",
    "    \"modelName\":model_name,\n",
    "    \"featureList\":feature_X_list,\n",
    "    \"target\": feature_y_list,\n",
    "    \"trainDataType\":trainDataType,\n",
    "    \"modelPurpose\":model_purpose,\n",
    "    \"modelMethod\":model_method,\n",
    "    \"modelTags\":modelTags,\n",
    "    \"modelCleanLevel\":model_clean,\n",
    "    \"trainParameter\": trainParameter,\n",
    "    \"modelParameter\": modelParameter,\n",
    "    \"transformParameter\":transformParameter,\n",
    "    \"scalerParam\":scalerParam,\n",
    "    \"trainDataName\":[data_name_X, data_name_y], \n",
    "\n",
    "    \"files\":{\n",
    "        \"modelFile\":{\n",
    "            \"fileName\":\"model.pth\",\n",
    "            \"filePath\":modelFilePath\n",
    "        },\n",
    "        \"XScalerFile\":{\n",
    "            \"fileName\":\"scaler.pkl\",\n",
    "            \"filePath\":X_scalerFilePath       \n",
    "        },\n",
    "        \"yScalerFile\":{\n",
    "            \"fileName\":\"scaler.pkl\",\n",
    "            \"filePath\":y_scalerFilePath      \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "modelInfoMeta = ml_meta.save_model_meta_data(mongo_client, modelInfoMeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a0c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e92cea83a25a22cd774ff9f8132db57ccb94d86fd97b7fe80ee00c35daecdd05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
